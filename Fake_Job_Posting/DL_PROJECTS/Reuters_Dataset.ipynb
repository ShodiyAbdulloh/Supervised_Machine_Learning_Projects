{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reuters Dataset module and Performance"
      ],
      "metadata": {
        "id": "rTR-xMjpwvml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfpHbX6zsbKB",
        "outputId": "aca690a7-2db8-4ad4-fd7c-f13efeb68c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "\u001b[1m2110848/2110848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n",
        " num_words=10000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "NzMLVRAntOvL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of samples in the training and test datasets,"
      ],
      "metadata": {
        "id": "ywMXZGvCxEd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data),len(test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uul7yoyGtOtB",
        "outputId": "27a882e0-1609-4748-fa4f-b1a63b6c68df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8982, 2246)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " labels (categories)"
      ],
      "metadata": {
        "id": "RKBAlZ38xcG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz_hoSj3tOqI",
        "outputId": "10c6df0f-9349-46f0-db08-eb0dbb58bb0c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8982,)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mHfXF_ctOm9",
        "outputId": "d92004b0-b31f-40d6-edb5-e52dce394854"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 3267,\n",
              " 699,\n",
              " 3434,\n",
              " 2295,\n",
              " 56,\n",
              " 2,\n",
              " 7511,\n",
              " 9,\n",
              " 56,\n",
              " 3906,\n",
              " 1073,\n",
              " 81,\n",
              " 5,\n",
              " 1198,\n",
              " 57,\n",
              " 366,\n",
              " 737,\n",
              " 132,\n",
              " 20,\n",
              " 4093,\n",
              " 7,\n",
              " 2,\n",
              " 49,\n",
              " 2295,\n",
              " 2,\n",
              " 1037,\n",
              " 3267,\n",
              " 699,\n",
              " 3434,\n",
              " 8,\n",
              " 7,\n",
              " 10,\n",
              " 241,\n",
              " 16,\n",
              " 855,\n",
              " 129,\n",
              " 231,\n",
              " 783,\n",
              " 5,\n",
              " 4,\n",
              " 587,\n",
              " 2295,\n",
              " 2,\n",
              " 2,\n",
              " 775,\n",
              " 7,\n",
              " 48,\n",
              " 34,\n",
              " 191,\n",
              " 44,\n",
              " 35,\n",
              " 1795,\n",
              " 505,\n",
              " 17,\n",
              " 12]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset General Information"
      ],
      "metadata": {
        "id": "hSzENOfbyK5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def dataset_info(data, labels):\n",
        "    print(\"Dataset Information:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Number of samples: {len(data)}\")\n",
        "    print(f\"Number of unique labels: {len(np.unique(labels))}\")\n",
        "    print(f\"First sample (decoded): {data[0]}\")\n",
        "    print(f\"First label: {labels[0]}\")\n",
        "    print(f\"Max sequence length: {max(len(sequence) for sequence in data)}\")\n",
        "    print(f\"Min sequence length: {min(len(sequence) for sequence in data)}\")\n",
        "    print(f\"Average sequence length: {np.mean([len(sequence) for sequence in data]):.2f}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Display training dataset info\n",
        "print(\"Training Data Info:\")\n",
        "dataset_info(train_data, train_labels)\n",
        "\n",
        "# Display test dataset info\n",
        "print(\"Test Data Info:\")\n",
        "dataset_info(test_data, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugiZ4Q8qtoSI",
        "outputId": "8180e225-9a1b-4248-c40c-15820b06cf71"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Info:\n",
            "Dataset Information:\n",
            "----------------------------------------\n",
            "Number of samples: 8982\n",
            "Number of unique labels: 46\n",
            "First sample (decoded): [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
            "First label: 3\n",
            "Max sequence length: 2376\n",
            "Min sequence length: 13\n",
            "Average sequence length: 145.54\n",
            "----------------------------------------\n",
            "Test Data Info:\n",
            "Dataset Information:\n",
            "----------------------------------------\n",
            "Number of samples: 2246\n",
            "Number of unique labels: 46\n",
            "First sample (decoded): [1, 4, 1378, 2025, 9, 697, 4622, 111, 8, 25, 109, 29, 3650, 11, 150, 244, 364, 33, 30, 30, 1398, 333, 6, 2, 159, 9, 1084, 363, 13, 2, 71, 9, 2, 71, 117, 4, 225, 78, 206, 10, 9, 1214, 8, 4, 270, 5, 2, 7, 748, 48, 9, 2, 7, 207, 1451, 966, 1864, 793, 97, 133, 336, 7, 4, 493, 98, 273, 104, 284, 25, 39, 338, 22, 905, 220, 3465, 644, 59, 20, 6, 119, 61, 11, 15, 58, 579, 26, 10, 67, 7, 4, 738, 98, 43, 88, 333, 722, 12, 20, 6, 19, 746, 35, 15, 10, 9, 1214, 855, 129, 783, 21, 4, 2280, 244, 364, 51, 16, 299, 452, 16, 515, 4, 99, 29, 5, 4, 364, 281, 48, 10, 9, 1214, 23, 644, 47, 20, 324, 27, 56, 2, 2, 5, 192, 510, 17, 12]\n",
            "First label: 3\n",
            "Max sequence length: 1032\n",
            "Min sequence length: 2\n",
            "Average sequence length: 147.66\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoding"
      ],
      "metadata": {
        "id": "6uonyB_1yV7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoding\n",
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict(\n",
        " [(value, key) for (key, value) in word_index.items()])\n",
        "decoded_newswire = \" \".join(\n",
        " [reverse_word_index.get(i - 3, \"?\") for i in train_data[5]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKHFdrTktoOv",
        "outputId": "fd6c1654-c2cd-4bfc-e165-9ace92c4db8e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
            "\u001b[1m550378/550378\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_newswire"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "2Kb-tiQCtoKw",
        "outputId": "b5231573-23d9-4b5a-f757-1c02499fe537"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"? the u s agriculture department estimated canada's 1986 87 wheat crop at 31 85 mln tonnes vs 31 85 mln tonnes last month it estimated 1985 86 output at 24 25 mln tonnes vs 24 25 mln last month canadian 1986 87 coarse grain production is projected at 27 62 mln tonnes vs 27 62 mln tonnes last month production in 1985 86 is estimated at 24 95 mln tonnes vs 24 95 mln last month canadian wheat exports in 1986 87 are forecast at 19 00 mln tonnes vs 18 00 mln tonnes last month exports in 1985 86 are estimated at 17 71 mln tonnes vs 17 72 mln last month reuter 3\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# turli xil lenga ega shuning uchun uni encoding qilish kerak\n",
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))  # Initialize a matrix of zeros\n",
        "\n",
        "    # Loop over each sequence\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        # Set the corresponding indices to 1\n",
        "        for j in sequence:\n",
        "            results[i, j] = 1.\n",
        "    return results"
      ],
      "metadata": {
        "id": "fbOhMzG6toHg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_train vs X_tesga ajraib olish"
      ],
      "metadata": {
        "id": "ATSYn9N-yavt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "metadata": {
        "id": "Fu7Vo7kqtoEE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Implementing one-hot encoding for the train_labels and test_labels."
      ],
      "metadata": {
        "id": "EqY-2utfyu1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def to_one_hot(labels, dimension=46):\n",
        "    # Create a matrix of shape (num_samples, dimension) initialized to zeros\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.  # Set the corresponding label index to 1\n",
        "\n",
        "    return results\n",
        "\n",
        "# Assuming `train_labels` and `test_labels` are the labels for training and test sets\n",
        "y_train = to_one_hot(train_labels)  # Convert training labels to one-hot\n",
        "y_test = to_one_hot(test_labels)    # Convert test labels to one-hot"
      ],
      "metadata": {
        "id": "5sqbnf2tucQk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical function from TensorFlow/Keras to convert the labels (train_labels and test_labels) into one-hot encoded format"
      ],
      "metadata": {
        "id": "C5t1Rq52zHGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(train_labels)\n",
        "y_test = to_categorical(test_labels)"
      ],
      "metadata": {
        "id": "_0-AQQU4ucML"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential([\n",
        " layers.Dense(64, activation=\"relu\"),\n",
        " layers.Dense(64, activation=\"relu\"),\n",
        " layers.Dense(46, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "2_xVr4zNucH4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5GQH5uDucDN",
        "outputId": "32a4118d-583d-4ed3-80ca-2400010b3243"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Sequential name=sequential, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        " loss=\"categorical_crossentropy\",\n",
        " metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "tX-cDpnzub9A"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "y_val = y_train[:1000]\n",
        "partial_y_train = y_train[1000:]"
      ],
      "metadata": {
        "id": "KoMlfKs3ub4p"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partial_x_train and partial_y_train are used as the training data, and x_val and y_val are used as the validation data."
      ],
      "metadata": {
        "id": "CybMd558z2Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(partial_x_train,\n",
        "partial_y_train,\n",
        "epochs=5,\n",
        "batch_size=512,\n",
        "validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsq6gf1Hub1E",
        "outputId": "ada923e7-36f1-45c0-b552-275556419090"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.3422 - loss: 3.3027 - val_accuracy: 0.5890 - val_loss: 1.9848\n",
            "Epoch 2/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.6562 - loss: 1.7555 - val_accuracy: 0.6880 - val_loss: 1.4093\n",
            "Epoch 3/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.7322 - loss: 1.2524 - val_accuracy: 0.7320 - val_loss: 1.1889\n",
            "Epoch 4/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 0.7841 - loss: 1.0013 - val_accuracy: 0.7600 - val_loss: 1.1020\n",
            "Epoch 5/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.8160 - loss: 0.8426 - val_accuracy: 0.7870 - val_loss: 1.0091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(46, activation=\"softmax\")  # 46 classes for multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",  # Use categorical crossentropy for multi-class\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=9, batch_size=512)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "results = model.evaluate(x_test, y_test)\n",
        "print(\"Test loss, Test accuracy:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvKGChecu6bp",
        "outputId": "087c9537-9d00-44b4-d76e-f2073fe7bdcb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.4223 - loss: 3.1228\n",
            "Epoch 2/9\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.6656 - loss: 1.5857\n",
            "Epoch 3/9\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7467 - loss: 1.1913\n",
            "Epoch 4/9\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.7937 - loss: 0.9569\n",
            "Epoch 5/9\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8407 - loss: 0.7738\n",
            "Epoch 6/9\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8682 - loss: 0.6265\n",
            "Epoch 7/9\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8954 - loss: 0.5278\n",
            "Epoch 8/9\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9168 - loss: 0.4243\n",
            "Epoch 9/9\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.9206 - loss: 0.3772\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8034 - loss: 0.9018\n",
            "Test loss, Test accuracy: [0.9271695613861084, 0.7943009734153748]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " randomized accuracy between two versions of the test_labels array."
      ],
      "metadata": {
        "id": "OJnIM6PZ0F2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " import copy\n",
        ">>> test_labels_copy = copy.copy(test_labels)\n",
        ">>> np.random.shuffle(test_labels_copy)\n",
        ">>> hits_array = np.array(test_labels) == np.array(test_labels_copy)\n",
        ">>> hits_array.mean()\n",
        "0.18655387355298308"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at8dF4a_u6MZ",
        "outputId": "2fee4449-f439-4899-83cd-d67698449bfe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18655387355298308"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression"
      ],
      "metadata": {
        "id": "zX6BPxnIvbL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasetni Tortib olamiz"
      ],
      "metadata": {
        "id": "vzemxnNG0MSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) = (\n",
        " boston_housing.load_data())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrJoDHhIu6IA",
        "outputId": "8618ffc2-d2b6-4e2c-ebe8-8236e7620439"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "\u001b[1m57026/57026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA INFO"
      ],
      "metadata": {
        "id": "LvLWOeEg0o-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyqcWehdu6EO",
        "outputId": "6a48820b-dc65-4fc5-e5d3-16edd3ca2c5a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(404, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA INFO"
      ],
      "metadata": {
        "id": "ivIxw8ve0r0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHNP2Syxu6AN",
        "outputId": "92ebe6a9-ccca-4c70-d0e7-2d913acc1e47"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Feature scaling"
      ],
      "metadata": {
        "id": "OW9aLWBj0mZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "test_data -= mean\n",
        "test_data /= std"
      ],
      "metadata": {
        "id": "xdQxoJcwvyA5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying loss function"
      ],
      "metadata": {
        "id": "ZSO4XmJG1I_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        " model = keras.Sequential([\n",
        " layers.Dense(64, activation=\"relu\"),\n",
        " layers.Dense(64, activation=\"relu\"),\n",
        " layers.Dense(1)\n",
        " ])\n",
        " model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        " return model"
      ],
      "metadata": {
        "id": "lKS7Ryyevx7T"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-fold Cross-Validation"
      ],
      "metadata": {
        "id": "H_h5-s-61ZsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 4  # Number of folds\n",
        "num_val_samples = len(train_data) // k  # Validation sample size per fold\n",
        "num_epochs = 100  # Number of epochs to train the model\n",
        "all_scores = []  # List to store scores (mean absolute error)\n",
        "\n",
        "for i in range(k):\n",
        "    print(f\"Processing fold #{i}\")\n",
        "\n",
        "    # Create validation data for this fold\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "\n",
        "    # Create training data for this fold\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [train_targets[:i * num_val_samples],\n",
        "         train_targets[(i + 1) * num_val_samples:]],\n",
        "        axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THMw8nDYvx2M",
        "outputId": "5233c0d2-76d7-4de3-a2a2-f1cebb9c35de"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing fold #0\n",
            "Processing fold #1\n",
            "Processing fold #2\n",
            "Processing fold #3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Absolute Error (MAE)"
      ],
      "metadata": {
        "id": "Jtywff3h10kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Build and compile the model (you need to define build_model function)\n",
        "    model = build_model()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(partial_train_data, partial_train_targets,\n",
        "              epochs=num_epochs, batch_size=16, verbose=0)\n",
        "\n",
        "    # Evaluate the model on the validation data\n",
        "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "\n",
        "    # Append the validation MAE score for this fold\n",
        "    all_scores.append(val_mae)\n",
        "\n",
        "# You can now print or return the mean of all scores\n",
        "print(f\"Mean MAE: {np.mean(all_scores)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8BAjctkvxyD",
        "outputId": "1c088208-0de9-4180-b1a3-cb82449f90f2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean MAE: 2.4078874588012695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "all_scores = [2.112449, 3.0801501, 2.6483836, 2.4275346]\n",
        "np.mean(all_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGx-a66Vvxtb",
        "outputId": "df2d8b83-a1a0-4fa9-be0f-23f15026ac07"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.567129325"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}